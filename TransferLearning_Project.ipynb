{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224, 224]\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`load_weights` requires h5py when loading weights from HDF5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e784c598979b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# include_top false means it removes top dense layers which are optimized for VGG images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# we need to add new layers and train with our data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# here [3] is 3 channels Red blue Green\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    218\u001b[0m           \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m           file_hash='6d6bbae143d832006294945121d1f1fc')\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2344\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2346\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m   2347\u001b[0m             '`load_weights` requires h5py when loading weights from HDF5.')\n\u001b[1;32m   2348\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `load_weights` requires h5py when loading weights from HDF5."
     ]
    }
   ],
   "source": [
    "# initializing VGG with its own weights,\n",
    "# include_top false means it removes top dense layers which are optimized for VGG images \n",
    "# we need to add new layers and train with our data.\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False) # here [3] is 3 channels Red blue Green\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-977254747b8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Here we Froze the weights of VGG- Meaning we are using existing features from VGG adn mixing with our dense layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vgg' is not defined"
     ]
    }
   ],
   "source": [
    "for layer in vgg.layers:\n",
    "  layer.trainable = False\n",
    "#Here we Froze the weights of VGG- Meaning we are using existing features from VGG adn mixing with our dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-12af781ec2d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model without any dense layers, features only from VGNET and direct predictions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# x = Dense(1000, activation='relu')(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# here we have 5 output classes so i gave 5 in output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vgg' is not defined"
     ]
    }
   ],
   "source": [
    "# Model without any dense layers, features only from VGNET and direct predictions.\n",
    "x = Flatten()(vgg.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "prediction = Dense(5, activation='softmax')(x) # here we have 5 output classes so i gave 5 in output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e397cbb64fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating VGNET model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vgg' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating VGNET model\n",
    "model = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-659acec4984f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.compile(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=tf.keras.optimizers.RMSprop(),\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 214 images belonging to 5 classes.\n",
      "Found 81 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#Input needs to be (224,224)\n",
    "# we are addign data augmentation to zoom in out , and flipping horizontally\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)# add augments with horizontal flip\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)#divide by 255 to normalize/scale\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('SecondDataset/Train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('SecondDataset/Test',\n",
    "                                           target_size = (224, 224),\n",
    "                                             batch_size = 32,\n",
    "                                             class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 8s 982ms/step - loss: 4.6014 - accuracy: 0.2850 - val_loss: 2.4125 - val_accuracy: 0.4198\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 7s 934ms/step - loss: 2.3912 - accuracy: 0.4065 - val_loss: 0.9441 - val_accuracy: 0.6049\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 7s 942ms/step - loss: 1.7323 - accuracy: 0.4533 - val_loss: 1.0046 - val_accuracy: 0.5556\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 7s 909ms/step - loss: 1.2578 - accuracy: 0.5654 - val_loss: 1.8019 - val_accuracy: 0.5309\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 7s 957ms/step - loss: 1.1292 - accuracy: 0.6822 - val_loss: 0.8106 - val_accuracy: 0.5926\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=5,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 542ms/step - loss: 0.8106 - accuracy: 0.5926\n",
      "Test Loss: 0.810613214969635\n",
      "Test accuracy: 0.5925925970077515\n"
     ]
    }
   ],
   "source": [
    "#This did not work very will with VG net architecture.\n",
    "#Transfer learning may not always work. we need to Tweak some parameters and try it agian,\n",
    "score = model.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am adding more train layers to customize the model more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "x=Dense(100, activation='relu')(x)\n",
    "Dropout(0.5)(x)\n",
    "x=Dense(100, activation='relu')(x)\n",
    "prediction = Dense(5, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               2508900   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 17,234,193\n",
      "Trainable params: 2,519,505\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=tf.keras.optimizers.RMSprop(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "callback = [earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 7s 977ms/step - loss: 6.1181 - accuracy: 0.2243 - val_loss: 1.8227 - val_accuracy: 0.2099\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 7s 885ms/step - loss: 1.8560 - accuracy: 0.2196 - val_loss: 1.4662 - val_accuracy: 0.4074\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 6s 905ms/step - loss: 1.4246 - accuracy: 0.4019 - val_loss: 1.0858 - val_accuracy: 0.4815\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 6s 858ms/step - loss: 1.5654 - accuracy: 0.3598 - val_loss: 1.0044 - val_accuracy: 0.4444\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 7s 902ms/step - loss: 1.0257 - accuracy: 0.5514 - val_loss: 1.0359 - val_accuracy: 0.6296\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 7s 890ms/step - loss: 1.5406 - accuracy: 0.4299 - val_loss: 0.9891 - val_accuracy: 0.5309\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 6s 890ms/step - loss: 1.0405 - accuracy: 0.5280 - val_loss: 0.8802 - val_accuracy: 0.5679\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 7s 937ms/step - loss: 0.7719 - accuracy: 0.7009 - val_loss: 0.5888 - val_accuracy: 0.7778\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 6s 905ms/step - loss: 0.7698 - accuracy: 0.7103 - val_loss: 1.0212 - val_accuracy: 0.6543\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 6s 906ms/step - loss: 0.9786 - accuracy: 0.6402 - val_loss: 0.4226 - val_accuracy: 0.9383\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 6s 862ms/step - loss: 0.6258 - accuracy: 0.8037 - val_loss: 0.6037 - val_accuracy: 0.8025\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 6s 881ms/step - loss: 0.6314 - accuracy: 0.7383 - val_loss: 0.3625 - val_accuracy: 0.9136\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 7s 899ms/step - loss: 0.6682 - accuracy: 0.7336 - val_loss: 0.4586 - val_accuracy: 0.8519\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 6s 867ms/step - loss: 0.4805 - accuracy: 0.8224 - val_loss: 0.5616 - val_accuracy: 0.7407\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 6s 891ms/step - loss: 0.5765 - accuracy: 0.7804 - val_loss: 0.7286 - val_accuracy: 0.6790\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 6s 869ms/step - loss: 0.5765 - accuracy: 0.7570 - val_loss: 0.4251 - val_accuracy: 0.8765\n"
     ]
    }
   ],
   "source": [
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=16,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set),callbacks=callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding more layers which are trainablle did not help our model.We need to try a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 453ms/step - loss: 0.4251 - accuracy: 0.8765\n",
      "Test Loss: 0.4251287877559662\n",
      "Test accuracy: 0.8765432238578796\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thrid Model - Now i will only use one dense layer and one for output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                1254450   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 15,969,393\n",
      "Trainable params: 1,254,705\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "# x = Dense(1000, activation='relu')(x)\n",
    "x=Dense(50, activation='relu')(x)\n",
    "prediction = Dense(5, activation='softmax')(x)\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=tf.keras.optimizers.RMSprop(),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "callback = [earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 7s 945ms/step - loss: 1.6366 - accuracy: 0.3178 - val_loss: 1.4621 - val_accuracy: 0.3457\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 6s 855ms/step - loss: 1.4594 - accuracy: 0.3738 - val_loss: 1.4445 - val_accuracy: 0.3210\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 6s 847ms/step - loss: 1.5083 - accuracy: 0.3411 - val_loss: 1.4679 - val_accuracy: 0.3210\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 6s 867ms/step - loss: 1.4596 - accuracy: 0.3692 - val_loss: 1.4136 - val_accuracy: 0.3580\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 6s 872ms/step - loss: 1.4367 - accuracy: 0.3598 - val_loss: 1.3927 - val_accuracy: 0.3580\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 6s 872ms/step - loss: 1.4389 - accuracy: 0.3598 - val_loss: 1.3856 - val_accuracy: 0.3457\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 7s 902ms/step - loss: 1.4218 - accuracy: 0.3785 - val_loss: 1.3906 - val_accuracy: 0.3580\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 6s 902ms/step - loss: 1.5248 - accuracy: 0.3458 - val_loss: 1.3544 - val_accuracy: 0.3580\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 6s 898ms/step - loss: 1.4362 - accuracy: 0.3598 - val_loss: 1.3438 - val_accuracy: 0.3704\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 6s 877ms/step - loss: 1.4734 - accuracy: 0.3785 - val_loss: 1.3500 - val_accuracy: 0.4074\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 6s 911ms/step - loss: 1.3782 - accuracy: 0.3551 - val_loss: 1.2065 - val_accuracy: 0.4321\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 6s 884ms/step - loss: 1.2822 - accuracy: 0.4112 - val_loss: 1.0868 - val_accuracy: 0.5556\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 6s 864ms/step - loss: 1.0854 - accuracy: 0.4579 - val_loss: 0.8836 - val_accuracy: 0.6296\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 6s 889ms/step - loss: 0.9753 - accuracy: 0.5794 - val_loss: 0.7956 - val_accuracy: 0.6914\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 6s 893ms/step - loss: 0.9303 - accuracy: 0.6215 - val_loss: 0.9792 - val_accuracy: 0.4938\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 6s 867ms/step - loss: 0.8455 - accuracy: 0.6589 - val_loss: 0.6053 - val_accuracy: 0.8272\n"
     ]
    }
   ],
   "source": [
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=16,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set),callbacks=callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 468ms/step - loss: 1.2659 - accuracy: 0.3086\n",
      "Test Loss: 1.2658671140670776\n",
      "Test accuracy: 0.3086419701576233\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model Fitted Best compared to the model whit direct predictions of our data with VGG \n",
    "#it worked better than the one with 2 dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its good to use simple models than complex models with more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we explored the Power of transfer learning, by using existing features from VGG\n",
    "# and adding only one dense layer other than output we saved a lot of compute power and time and\n",
    "#achieved great accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
